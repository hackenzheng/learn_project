此目录含socket编程，网络编程等相关知识，不区分语言

同步异步：

    (1)在《unix网络编程》书中给出了阻塞、非阻塞、io复用、信号驱动、异步５种io模型，并把前面四种归结为同步io,
    　　　因为每个read的时候数据从内核拷贝到用户的缓冲区仍然会阻塞. 这里的同步异步指的是socket io的特点
    (2)同步编程和异步编程：指的是多个io调用之间是否按照顺序来，比如client端向server端发数据，要先connect，再write
    　　同步过程就是先等connect连接成功了，无论是阻塞还是非阻塞io(阻塞io进程会挂起也做不了其他事，非阻塞则要进行处理，
    　　在循环中一直判断，直到连接成功才退出循环)，异步是先connet，不管是否连接成功就直接write数据，如果没连接成功write
    　　会返回错误需要处理，直到返回正确。异步的编程要复杂些，但效率高，当一个io调用没准备好立即操作下一个。
    
    同步异步， 同步是当前线程如果碰到io就会阻塞等待，如果是异步，不会阻塞等待，而去执行另外的任务，异步会是无序的，
    需要每个任务间是独立的，适用场景是任务简单，但是对性能要求高的场景
    同步io和异步io，是对io的描述，而同步编程和异步编程是多任务处理的编程方式，同步是需要协调，异步是无需协调
    非阻塞提高整体的执行效率，异步是高效的组织非阻塞任务，所以异步必然是在非阻塞的基础上，如果阻塞就挂起了，谈不上异步.
    单个的io请求使用非阻塞模式并没有提高效率  

客户端的连接数取决于端口数, 但是一块网卡可以配置多个ip, 那么不同ip是可以配置相同的端口.
服务端的连接数取决于文件描述符的限制,内存, 但文件描述符是可以改的,理论上取决于服务器的性能即cpu和内存.
如果性能够,那么并发的连接数是所有可能的客户端发出的连接数,那就是2的48次方.

计算机中不同操作耗时水平: 锁是一个很耗时的东西,互斥锁的加锁和解锁时间需要 25ns, 一次 CPU 上下文切换（如系统调用,进程切换）需要大约1500ns.
上下文切换更恐怖的事情在于，这段时间里 CPU 没有做任何有用的计算，只是切换了两个不同进程的寄存器和内存状态；
而且这个过程还破坏了缓存，让后续的计算更加耗时。从世界上不同城市网络上走一个来回，平均需要 150ms 

文件io的过程：write() -> write系统调用 -> 内核中对磁盘的缓冲 -> 写入到磁盘  因为每次read都会调一次系统调用，会有内核态和用户态之间的切换，影响效率。
所以使用带缓冲的io,将多次写操作缓冲起来然后再调一次read提高效率。 标准io中的缓冲分全缓冲和行缓冲,行缓冲常用于标准输出，一个stdout
文件io打开文件后返回的是文件描述符，标准io打开问加你之后返回的是文件流FILE变量， fopen打开文件，建立一个文件流，与文件关联

 
文件操作两种， 一种是直接操作，系统调用，直接对文件描述符操作，文件描述符是整型int fd
一种是带缓冲的，将文件描述符与缓冲绑定，直接操作的是缓冲，操作的是文件指针FILE *, FILE结构包括一个缓冲区和一个文件描述符。FILE *比fd更适合跨平台
 
open是直接的系统调用，返回文件描述符， fopen封装了open，增加了缓冲，fopen默认缓冲，返回FILE *,fopen是直接打开文件作为FILE *, 
fdopen是将已经打开的文件描述符转为FILE *,可以从 open，dup，dup2，fcntl，pipe，socket，socketpair或accept函数得到此文件描述符 
FILE结构在不同操作系统不一样，参考https://www.cnblogs.com/wxl845235800/p/7241020.html


accept是创建一个新的socket，Linux下默认创建的socket是阻塞的。无论linsten socket是阻塞还是非阻塞，返回的都是非阻塞的socket。
socket可以设定监听地址为255.255.255.255，这样是连接不上的。


进程间同步与互斥用信号量,不同进程间访问同一个资源之间的同步也可以用互斥锁（需要设置进程共享）。 另外有文件锁,专门针对文件操作。
进程间的锁是实现同一服务器上不同进程间的同步，分布式锁是实现不同服务器上的不同进程之间的同步。
可重入锁，也叫做递归锁，指的是在同一线程内，外层函数获得锁之后，内层递归函数仍然可以获取到该锁。
可重入锁可以防止在同一线程中多次获取锁而导致死锁发生。

自旋锁应用在锁住时间短的场景，没有获取到锁会轮询，不会阻塞，而互斥锁没有获取到是会阻塞线程，会有两次上下文切换。
如果自旋等待获取到锁的时间小于上下文切换时间，就是有优势的。用户层较少用到自旋锁。
条件变量是一个线程等待另一个线程准备好条件,而锁是限制对共享资源的访问。

锁是用于线程间互斥访问共享资源的,信号量是用于线程间同步的,互斥量就是锁,又叫互斥锁,互斥是同步的一种特例, 所以二进制信号量与锁的功能一样.
线程运行结束,若信号量不释放,其他线程是拿不到,会被继续阻塞, 导致死锁.
普通信号量可以被多次释放,不会抛异常,会造成信号量比初始值大, 所以有了BoundedSemaphore即有界信号,避免多次释放.

Ubuntu等linux是分时操作系统,单片机上才跑实时操作系统，因为不要与多个用户交互，更多的是做控制,要及时响应.
进程调度需要解决什么时候开始调度,可以是时间片到了调度到新进程,可以是时间片没到因为有触发信号调度新进程,比如当前进程阻塞.
但如果当前进程正在处理终端,或者执行原子操作等，则必须处理完之后才会进行调度。
如果高优先级的任务会打断低优先的任务那么就是可抢占（可剥夺）,可抢占又分为内核可抢占和用户可抢占。


信号是进程间的通讯方式, 既然是进程间, 那么信号发送方也可以是进程,  也可以是内核检测到事件后发送信号, 可以自定义信号接收到以后的行为.
进程中可通过os.kill()系统调用来发送信号, kill不是直接杀死进程,只是一个系统调用, 会传递信号. 类似shell下的kill命令,
发送信号一般有两种原因:
1(被动式)  内核检测到一个系统事件.例如子进程退出会像父进程发送SIGCHLD信号.键盘按下control+c会发送SIGINT信号
2(主动式)  通过系统调用kill来向指定进程发送信号
操作系统规定了进程收到信号以后的默认行为但是，我们可以通过绑定信号处理函数来修改进程收到信号以后的行为有两个信号是不可更改的SIGTOP和SIGKILLkill命令-9才是强制杀死进程




## io复用


阻塞io是在io执行的两个阶段(等待数据到来d和拷贝数据)都被阻塞.阻塞io,非阻塞io,多路复用io都是属于同步io, 因为在都含等待数据和准备数据,
在内核准备数据的阶段是会阻塞的. 而异步io是数据到来然后再拷贝到用户进程的整个过程都不会被阻塞.
多路复用io,也称为事件驱动io.select调用是阻塞的,所以比阻塞io模型并没有太大不同,因为相比还多了一个调用.但是select的优势并不是对
单个连接处理更快,而是在于能处理更多的连接.在多路复用io中,socket一般都设置为非阻塞的.

select原理是把文件描述符集合拷贝到内核，然后内核进行轮询，每次调用都需要拷贝
而epoll是注册的方式，是一个个添加，每次只要添加一个文件描述符，epoll每次返回的都是有数据的，select返回只是表示有事件，但具体哪个可以读需要自行判断
epoll wait只是读取有用的，在从上次到这一次读取之间的这段事件，描述符要是就绪了就通过回调加入到rdlist里面，提高io效率
而select是每次内核都会轮询每以描述符，看有没有事件，
select的返回值是用于判断是出错还是可以读，然后通过传进去的fds逐个判断哪个可以操作
连接数少且都十分活跃的情况下，select是比较好的，因为epoll的实现机制中有函数回调，有一定开销
select受限于文件描述符大小的限制，通过重新编译内核可以扩大。 poll也是轮询，但是内部是基于链表存储的，没有大小限制。
如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd
epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知



## tcp状态

服务端只要listen之后就能够监听客户端的请求你，比如在listen和accept之间睡眠，此时客户端请求过来，
是可以建立连接的， accept只是从连接的队列中取出最早建立的

为什么要三次握手： 三次握手指的是握手需要三个包，服务端是要等第三个包到了才会认为是建立连接。如果服务端第一次收到syn就回
个ack就认为连接已建立，会出现服务端回的ack包没有到达，客户端会重发，那么重发的包会被认为是新的连接，而不是已有的连接。
这样带来的影响是浪费连接资源去等待数据到来，而且两个连接的五元组一样。  
第一个syn包重发的直接原因是第二个ack包没到来，没到来有两种情况一是发出去的syn包丢失了，服务端根本就不会回ack，
二是回的ack包丢失了,客户端也收不到。

    (1)  如果第一个包，A发送给B请求建立连接的报文(SYN)如果丢掉了，A会周期性的超时重传，直到B发出确认(SYN+ACK)；
    (2) 如果第二个包，B发送给A的确认报文(SYN+ACK)如果丢掉了，B会周期性的超时重传，直到A发出确认(ACK)；
    (3) 如果第三个包，A发送给B的确认报文(ACK)如果丢掉了，
    
    
    A在发送完确认报文之后，单方面会进入ESTABLISHED的状态，B还是SYN_RCVD状态
    如果此时双方都没有数据需要发送，B会周期性的超时发送(SYN+ACK)，直到收到A的确认报文(ACK)，此时B也进入ESTABLISHED状态，双方可以发送数据；
    如果A有数据发送，A发送的是(ACK+DATA)，B会在收到这个数据包的时候自动切换到ESTABLISHED状态，并接受数据(DATA)；
    如果这个时候B要发送数据，B是发送不了数据的，会周期性的超时重传(SYN+ACK)直到收到A的确认(ACK)B才能发送数据。



time_wait状态： 

    是什么：是主动关闭的一端有的状态，正常关闭的情况下就会出现； 
    作用是： (1)让ack能够到达对端，如果没有到达对端会重发fin包，所以需要等待再处理，即可靠地实现TCP全双工连接的终止; 
    (2)经过2MSL，上一次连接中所有因超时重发的包都会消失，避免影响下一个连接， 这个重发的包不只是关闭过程中重发的。
    也就是下一个连接的四元组即使都相同，不会出现上一个四元组的包来干扰。MSL是最长分节生命(maximum segment lifetime).
    每个ip数据包都有一个跳数限制(TTL)字段，尽管这是一个跳数限制字段而不是真正的时间限制，我们仍然假设具有最大跳数限制的分组
    在网络中存在的时间不可能超过2MSL.  也就是IP分组不会无限制的在网络中传输，如果超过一定的限制还没到路由器就不转发。
    带来的负面影响有： 服务端要为每个正常关闭的连接都要维护这么一个状态，降低了资源利用率，2msl目前一般是1分钟的时间。
    如果服务端出现大量time_wait状态怎么处理： 设置msl时间更小，或者设置端口复用。 time_wait是连接正常关闭的情况下也会出现，只能从系统层面优化，与代码无关。
    
    共有三个状态会进入time_wait状态： CLOSING,FIN_WAIT_1,FIN_WAIT_2
    
    <Unix 网络编程 TCP状态转换图详解> https://blog.csdn.net/wenqian1991/article/details/40110703
    
    具体的内核参数修改:
    vi /etc/sysctl.conf
    
    # 加入以下内容
    net.ipv4.tcp_syncookies = 1
    net.ipv4.tcp_tw_reuse = 1
    net.ipv4.tcp_tw_recycle = 1
    net.ipv4.tcp_fin_timeout = 30
    net.ipv4.tcp_timestamps
    
    /sbin/sysctl -p   让参数生效
    
    # 查看修改后的值
    cat /proc/sys/net/ipv4/tcp_syncookies
    cat /proc/sys/net/ipv4/tcp_tw_reuse
    cat /proc/sys/net/ipv4/tcp_tw_recycle
    cat /proc/sys/net/ipv4/tcp_timestamps
    
    net.ipv4.tcp_syncookies = 1表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
    net.ipv4.tcp_tw_reuse = 1表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭,是四元组完全相同
    net.ipv4.tcp_tw_recycle = 1表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
    net.ipv4.tcp_fin_timeout修改系統默认的TIMEOUT时间
    1、开启tcp_timestamp是开启tcp_tw_recycle，tcp_tw_reuse和tcp_timestamp的前提条件。
    2、但是在nat模式下，不用将tcp_tw_recycle和tcp_timestamp同时开启，这会造成tcp超时引发故障。
    
close_wait状态：
    
    被动关闭的一端再接收到FIN包之后就进入到该状态，然后回个ACK。在调用close之前都是这个状态。
    之所以叫 CLOSE_WAIT 可以理解为被动关闭的一方此时正在等待上层应用程序发出关闭连接指令即调用close().
    
    time_wait和close_wait状态的出现不能说是客户端和服务端，得是说主动关闭的一端和被动关闭的一端。
    因为客户端和服务端都可以主动关闭也都可以被动关闭。 一般的情况是服务端会处理多个连接，所以大量的time_wait状态和大量的close_wait
    状态连接的情况指的是服务端，除非客户端是做压力测试用或者做爬虫，会发起大量连接。
    
    close_wait状态的大量出现与代码有关，因为对端发了fin包要求结束，但自己这边如果不想结束就会一直处于这个状态。 但如果自己这边本来要结束，
    却没有结束即忘了close就去做其他的事情去了，进程没有立即结束，那么就会导致大量的close_wait出现。 比如在websocket中，客户端关闭了连接，
    但服务端这边没有close调，如果是单进程模式下，进程一直存活连接就一直不会释放。
    
    <服务器TIME_WAIT和CLOSE_WAIT详解和解决办法> https://www.cnblogs.com/sunxucool/p/3449068.html



## 端口复用
<Linux网络编程——端口复用（多个套接字绑定同一个端口）> https://blog.csdn.net/tennysonsky/article/details/44062173

复用真正的作用是在time_wait状态的处理